-- Sheet 1 --

Question 5

(a) A divide-and-conquer algorithm is one which is based on the divide-and-
conquer design strategy.
The divide-and-conquer design strategy solves a problem by:

1. Breaking the problem into "subproblems" - smaller instances of the same
problem
2. Recursively solving those subproblems
[Base case: If the subproblems become small enough, just solve them by brute
force.]
3. Appropriately combining their answers.


(b)
FIND-MIN-MAX(A, l, r)
Input: An integer array A with indices l < r
Ouput: A pair (mi, ma) = (min(A[l..r), max(A[l..r))
    
    // Base cases
    if r - l == 1
        return (A[l], A[l])
    if r - l == 2
        if A[l] < A[l + 1]
            return (A[l], A[l + 1])
        else
            return (A[l + 1], A[l])
    
    // Divide
    mid = (l + r) / 2
    (mi1, ma1) = FIND-MIN-MAX(A, l, mid)
    (mi2, ma2) = FIND-MIN-MAX(A, mid, r)
    
    // Conquer
    return (min(mi1, mi2), max(ma1, ma2))

To solve for the whole array, just call FIND-MIN-MAX(A, 0, N).


For the number of operations, consider the proof by induction for k >= 1:

P(k): The algorithm uses at most ceil(3 * 2^(k - 1)) - 2 comparisons.

Base case: k = 1 => 1 comparison is used <= 1 comparison, as expected

Inductive step: Assume P(k) holds, let's prove that P(k + 1) also holds.

The number of comparisons FIND-MIN-MAX(A, 0, 2^(k + 1)) uses is exactly 1 more
than twice the number of of comparisons used by FIND-MIN-MAX(A, 0, 2^k), in all
nontrivial cases. The inductive hypothesis tells us that this number is bounded
above by 2 + 2 * (ceil(3 * 2^k) - 2) = 2 * ceil(3 * 2^k) - 2 <= 
ceil(3 * 2^(k + 1)) - 2, as expected, and hence P(k + 1) holds.

As P(1) holds and P(k) => P(k + 1) for all k >= 1, by the principle of
mathematical induction, P(k) holds for all k >= 1, as required. 


(c) When d = log_b(a), T(n) = O(n^d*log(n))
    When d > log_b(a), T(n) = O(n^d)

Proceed by a Recursion Tree proof:

The total work done on the 0th level is O(n^d).
The total work done on the 1st level is O(a   * (n / b  )^d).
The total work done on the 2st level is O(a^2 * (n / b^2)^d).
...

Thus, for some constant c, T(n) <= c * n^d + a * c * (n / b)^d + a^2 * c *
                                   (n / b^2)^d + ...
Or, T(n) <= c * n^d * (1 + (a / b^d) + (a / b^d) ^ 2 + ...)

When d = log_b(a) <=> a = b ^ d all terms of the sum are equal to 1, and thus
T(n) <= c * n^d * log_b(a), as the total number of terms is log_b(a). Thus,
T(n) = O(n^d * log(n)).

When d > log_b(a) <=> a < b^d <=> a / b^d < 1, and thus the whole sum is
bounded by the geometric series sum:
T(n) <= c * n^d * (1 / (1 - a / b^d)). Thus T(n) = O(n^d).


(d)
FIND-LOCAL-MAX(A)
Input: An integer array A 
Output: An index p such that A[p] is a local maximum
    // Edge case
    if N == 1
        return 0

    // Check the boundaries
    if A[0] > A[1]
        return 0
    if A[N - 2] < A[N - 1]
        return N - 1
    
    // General case
    return FIND(A, 0, N)

FIND(A, l, r)
Input: An integer array A with two indices l < r
Output: An index p such that l < p < r - 1 and A[p] is a local maximum
Pre: A[l] < A[l + 1] and A[r - 2] > A[r - 1]

    p = (l + r) / 2
    if A[p - 1] > A[p]
        return FIND(A, l, p + 1)
    if A[p] < A[p + 1]
        return FIND(A, p, r)
    return p

Runtime: Let T(n) = T(n / 2) + O(1) be the runtime of FIND, which surely
         dominates the total runtime. This is O(log(N)) by the Master Theorem.
         
Correctness: When N <= 2 the edge-boundary cases always correctly
             determine a local maximum, so we may safely only consider N > 2.
             In this case FIND is called. 
             
             Lemma: Under the Precondition, FIND will always return an
                    appropriate local maximum.
             Proof: Strong induction on N:
                    Assume FIND works for all n < N, let's prove it also
                    works for N. To do this, let's look at FIND piecewise:
                    1. When the first if condition holds, FIND(A, l, p + 1) is
                       called with the precondition is satisfied, so, by the
                       ind. hypothesis FIND works in this case.
                    2. When the second if condition holds, FIND(A, p, r) is
                       called with the precondition is satisfied, so, by the
                       ind. hypothesis FIND works in this case, as well.
                    3. When none of the branches are executed, we know that
                       A[p - 1] < A[p] > A[p + 1], so, by definition p is a
                       local maximum, as expected.


................................................................................
        
                        
Question 6

(a) A binary min-heap is an essentially complete binary tree (i.e. it is
completely filled on all levels except possibly on the lowest, which is filled
from the left up to a point) that satisfies the Min-Heap Property (the value
of a node (except the root) is more than or equal to that of its parent).

For an example consider A = [4, 2, 10, 3, 7, 10, 10, 8]
Then HEAPIFY(A, 0, 8) will modify A as follows:

A = [4, 2, 10, 3, 7, 10, 10, 8] 
A = [2, 4, 10, 3, 7, 10, 10, 8] 
A = [2, 3, 10, 4, 7, 10, 10, 8]


(b) Min-priority-queues can be naturally implemented via min-heaps. All we
need to do is maintain both the element and the corresponding key in each
node of the underlying heap. So each array element will have 2 fields:
A[i].elem and A[i].key.

EXTRACT-MIN(A, n)
    min = A[0]
    swap(A[0], A[n - 1])
    n = n - 1
    HEAPIFY(A, 0, n)
    return min

DECREASE-KEY(A, i, key)
    A[i] = key
    while i > 0 and A[Parent(i)] > A[i]
        swap(A[i], A[Parent(i)])
        i = Parent(i)

Both operations run in time proportional to the height of the heap, or O(log(n))


(c)
MODIFIED_DIJKSTRA(G, s, w)
Input: Graph G = (V, E), source vertex s in V, and positive weights w: E -> R+
Output: The value of best[v], for all v in V.
    
    for each v in V
        d[v] = INF
        best[v] = 0
    d[S] = 0
    Q = MAKE-QUEUE(V) with (d[v], best[v]) as keys
    while Q != {}
        u = EXTRACT-MIN(Q)
        for each vertex v in Adj[u]
            aux = (d[u] + w(u, v), best[u] + 1)
            if aux < (d[v], best[v])
                (d[v], best[v]) = aux
                DECREASE-KEY(Q, v, d[v])

The complexity of the algorithm is the same as that of normal Dijkstra's
- O((|V| + |E|) * log |V|).

Worked example:

Initialization:
    Vertex s   a   b   c   d   e   f
    d      INF INF INF INF INF INF INF
    best   0   0   0   0   0   0   0
    
    Vertex s   a   b   c   d   e   f
    d      0   INF INF INF INF INF INF
    best   0   0   0   0   0   0   0 
    
    Q = {s with key (0,   0),
         a with key (INF, 0),
         ...
         f with key (INF, 0)}
    
Extract s:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   INF INF INF
    best   0   1   1   1   0   0   0
    
    Q = {a with key (2,   1),
         b with key (2,   1),
         c with key (4,   1),
         d with key (INF, 0),
         e with key (INF, 0),
         f with key (INF, 0)}
    
Extract a:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   INF INF INF
    best   0   1   1   1   0   0   0   
    
    Q = {b with key (2,   1),
         c with key (4,   1),
         d with key (INF, 0),
         e with key (INF, 0),
         f with key (INF, 0)}    
        
Extract b:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   INF INF INF
    best   0   1   1   1   0   0   0   
    
    Q = {c with key (4,   1),
         d with key (INF, 0),
         e with key (INF, 0),
         f with key (INF, 0)}
    
Extract c:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   5   5   7
    best   0   1   1   1   2   2   2
    
    Q = {d with key (5, 2),
         e with key (5, 2),
         f with key (7, 2)}
       
Extract d:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   5   5   6
    best   0   1   1   1   2   2   3
    
    Q = {e with key (5, 2),
         f with key (6, 3)}
         
Extract e:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   5   5   6
    best   0   1   1   1   2   2   3
    
    Q = {f with key (6, 3)}
    
Extract f:
    Vertex s   a   b   c   d   e   f
    d      0   2   2   4   5   5   6
    best   0   1   1   1   2   2   3
    
    Q = {}

................................................................................


Question 7

(a) A max-priority-queue is an abstract data structure for maintaining a set
of elements, each with an associated value called a key. It supports:

1. INSERT(S, x, k) - insert element x with key k into set S.
2. MAXIMUM(S) - returns the element of S with the largest key.
3. EXTRACT-MAX(S) - removes and returns the element of S with the largest key.
4. INCREASE-KEY(S, x, k) - increases value of x's key to k. Requires k to be
at least as large ast x's current key value.


(b) 1. INSERT can be implemented by appending it with key -INF to the back of 
the array maintaining the heap and then calling INCREASE-KEY-HEAP to set the
right key value. O(log(N)), where N is the size of the maintained set.
2. MAXIMUM can be implemented by simply returning the first element of the
array maintaining the heap. O(1)
3. EXTRACT-MAX - will be shown in pseudocode. O(1)
4. INCREASE-KEY(S, x, k) - assuming x actually denotes the index of the node
whose key we wish to increase, just call INCREASE-KEY-HEAP on that node with
the new key. O(log(N))

EXTRACT-MAX(S)
    max = S[0]
    S.heap-size = S.heap-size - 1
    S[0] = S[S.heap-size]
    HEAPIFY(S, 0)
    return max


(c) Excluding the initial S[0] = S[s.heap-size] line, there needs to be at
least one extra swap executed by the HEAPIFY function call (as otherwise
the last element in the array would have the second largest key, which, of
course, can not happen for a size 15 set). This bound is tight, as it can be
seen on:
S = [15, 14, 13, 6, 5, 12, 11, 4, 3, 2, 1, 10, 9, 8, 7]


(d) Build a max-heap of size k, whose entries represent the first k entries in
the given unsorted array. Then, for all remaining entries x in the unsorted
array, compare x with the largest element in the heap at that time, h:

1. If x < h, then extract the maximum out of the heap and insert x into the
heap.
2. Otherwise, if x >= h, continue.

At the end, the maximum element in the heap is the answer.
The method runs in time O(n*log(k)) because for each entry in the unsorted
array at most two costy heap operations are executed, namely EXTRACT-MAX and
INSERT, each running in time O(log(k)), where k is the (constant) size of the
heap.
Note: The initial cost of building the heap can be neglected, as it's not worse
      than executing an EXTRACT-MAX and INSERT operation pair which I considered
      for the analysis.


................................................................................


Question 8

(a) A spanning tree (ST) of a connected graph G is a subgraph of G, T = (V, E')
such that E' in E and T is a tree. Now, for weighted graphs, the cost of a SP
is defined as the sum of the weights of the edges in E'. A SP is called minimum
if and only if its cost is smallest amongst all SPs of G (MST).


(b) A pair (A, B) of 2 subsets of vertices A, B in V is called a cut if and
only if {A, B} partitions V. An edge e = (a, b) in E is said to cross the cut
(A, B) if and only if a in A and b in B or a in B and b in A.

Cut Lemma: For every cut (A, B) in a graph G, if e in E is one of the edges
of minimum weight crossing the cut (A, B), then there exists a MST T = (V, E')
such that e in E'.


(c) Simply consider the new weighting function c'(u, v) = -c(u, v) and run
Kruskal's using it. The found SP will be the maximum SP of the graph, as
required.


(d) I claim that the path in the maximum SP of the graph is always the path
we're seeking for. Computing this SP is easy, as explained in part (c). The
complexity of running Kruskal's is dominated by the sorting, which, under the
standard model, can not be done better than O(|E|log|V|).

Now, for the proof, consider the following worse incremental approach to the
problem, I'll show it's somehow equivalent to the modified Kruskal's we've used
to solve (c), thus completing the proof:

For 2 fixed vertices A, B we'll try to find the widest path between them.
Start with an empty graph and add the edges one by one, in decreasing order of
weight, stopping as soon as A and B become connected through a path. Observe
that this does, indeed, produce the widest path possible, as if there is no
path between A and B considering all edges >= C, then we'll need to lower the
prospective answer C until such a path emerges, following the described process.
Observe that we actually don't need all edges added by the procedure, only
those which join together 2 connected components ever being of any use. With
this last observation, we can see by literally comparing the 2 algorithms
that the edges which ever join 2 connected components are exactly those
added by the modified Kruskal into the returned SP, so the first
path to ever join A and B is exactly the path in the maximum SP, as required.


................................................................................
-- Sheet 2 --


Question 5

(a)
DIJKSTRA(V, E, w, s)
Input: A directed or undirected graph (V, E), s in V and w: E -> R>=0
Ouput: For each v in V, d[v] and pi[v]

    for each v in V
        d[v] = INF
        pi[v] = nil

    d[s] = 0
    Q = MAKE-QUEUE(V) with d[v] as keys
    while Q != {}
        u = EXTRACT-MIN(Q)
        for each v in Adj[u]
            if d[u] + w(u, v) < d[v]
                d[v] = d[u] + w(u, v)
                pi[v] = u
                DECREASE-KEY(Q, v, d[v])


(b) Let V = {1, 2, 3, 4} and E = {1 -> 2,
                                  2 -> 3,
                                  1 -> 3,
                                  3 -> 4} and
                                  w(1, 2) =  1000
                                  w(2, 3) = -1000
                                  w(1, 3) =     2
                                  w(3, 4) =     1

In this case Dijkstra's will mistakenly return d[4] = 3, when d[4] is
actually 1.


(c) Consider a graph G with V = {1, 2, 3, 4} and the edges:

a   b   w(a, b)
1   2      -1
2   3      -1
3   4      -1
1   4      -2

There are 2 paths between 1 and 4, P1: 1 -> 2 -> 3 -> 4 and P2: 1 -> 4. Denote
by w(P) the sum of the weights of the edges of path P, then:

        Before the addition    After the addition
w(P1)          -3                     3
                ^                     v
w(P2)          -2                     0

As it can be seen, in the initial graph P1 was the shortest path, while
afterwards P2 became the shortest path, so the proposed statement is false.
The actual fix is called Johnson's Algorithm ("Potentials Dijkstra's").


(d) In this analysis I will neglect all initializations, which are linear in the
size of the data structures(including MAKE-QUEUE if implemented properly), and
so don't change the final complexities.

    (i)  Each edge will be traversed exactly once, causing a O(1) array update.
         Each node will be the "minimum" exactly once, and O(|V|) time will be
         spent looking for the minimum each time.
         Overall, we get O(|E| + |V|^2)

    (ii) Each edge will be traversed exactly once, causing a O(log|V|) min-heap
         decrease key operation to be executed.
         Each node will be the "minimum" exactly once, and O(log|V|) time will
         be spend extracting the minimum from the min-heap.
         Overall, we get O((V + E)log|V|)


................................................................................


Question 6

(a) The divide-and-conquer design strategy solves a problem by:

1. Breaking the problem into "subproblems" - smaller instances of the same
problem
2. Recursively solving those subproblems
[Base case: If the subproblems become small enough, just solve them by brute
force.]
3. Appropriately combining their answers.

Mergesort is the typical example of applying the divide-and-conquer design
strategy for sorting:

1. Split the array into two (nearly) equal halves.
2. Recursively sort the two halves.
3. Use a linear time two arrays merge algorithm to produce the final sorted
   array.

The time complexity of Mergesort is O(NlogN), as required.


(b) First of all, proving this for permutations is enough.
I will only give the proof for deterministic algorithms within the standard
comparison-based model of computation:

Consider an algorithm that uses at most k comparisons for each one of the N!
possible permutations. WLOG, say it uses EXACTLY k comparisons every time.
For a fixed permutation p, call the "sign sequence" of p the length k string
consisting only of the characters '<' and '>' which resembles the results of the
comparisons made by the algorithm, in the order in which they were made. 

Assume for a contradiction that 2^k < N!, then, by the pigeonhole principle,
there are 2 permutations p1 and p2 with the same sign sequence. By the
deterministic nature of the algorithm, we know that, actually, it's
not only the case that the comparison results were the same, but also the
comparisons themselves were the same. This can only mean one thing, p1 and p2
are equivalent modulo this algorithm, so they will be sorted the same way.
But, it can not be the case that two different permutations p1 and p2 need the
exact same rearrangement to be sorted, so the algorithm must have committed a
mistake for either p1 or p2, a contradiction.

So, 2^k >= N!, so k >= log(N!), so k = Omega(log(N!)) = Omega(NlogN), as
required.


(c) (i)   Call Mergesort on the whole array and pick out the first i.
          Time complexity O(NlogN)
        
    (ii)  Implement the min-priority-queue as a min-heap. Building it takes
          time O(N), if done by repeatedly calling HEAPIFY in decreasing order
          of index. EXTRACT-MIN runs in time O(log N).
          Time complexity O(N + ilogN)
        
    (iii) Use the linear time selection algorithm (or the much faster in
          practice QUICK-SELECT, although randomized) on the whole array, then
          sort the first i entries with Mergesort.
          Time complexity O(N + ilogi)


................................................................................


Question 7

(a) Solving a problem by Dynamic programming is done by identifying a collection
of subprobems, tackling them in turn, smallest ones first, using the answer to
smaller problems to work out larger ones, until all of them are solved.
The principle of optimality: The optimal solution to a problem is a composite
of optimal solutions to (some of) its subproblems.


(b) The problem (No repetitions Knapsack):
A burglar finds much more loot than he had expected:
- His knapsack will hold a total of at most W kg.
- THere are n items to pick from, of weights w1 ... wn and values v1, ... vn.
- Each item can only be packed at most once.

Formally:
    Input: n, W, w1 ... wn and v1 ... vn.
    Output: The highest possible total value the of the loot the burglar may
            pack without exceeding the knapsack capacity W.

Consider dp[i][w] = the highest total value the burglar may 
                    pack up considering only items 1 ... i
                    using a knapsack of total capacity w.
Initialization:
    dp[0][w] = 0 for all w >= 0

Recurrence relation:
    - For w >= wi 
        dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - wi] + vi)
    - For w < wi
        dp[i][w] = dp[i - 1][w]


The subproblems will be solved in row-major order.
The final answer is dp[n][W].
The data structures needed are simple 1D and 2D arrays.
Complexity O(n * W).


(c) Consider dp[i][w1][w2] = the highest total value the burglar may pack up
                             considering only items 1 ... i using 2 knapsacks:
                             one of capacity w1 and another of capacity w2.
                             
Initialization:
    dp[0][w1][w2] = 0 for all w1, w2 >= 0
    
Recurrence relation: (for the simplicity of the description I will assume that
                      dp[i][<0][_] = dp[i][_][<0] = -INF)

dp[i][w1][w2] = max(dp[i - 1][w1][w2], dp[i - 1][w1 - wi][w2] + vi,
                                       dp[i - 1][w1][w2 - wi] + vi)


The subproblems will be solved in row-major order.
The final answer is dp[n][W1][W2].
Complexity O(n * W1 * W2).


................................................................................


Question 8

(a) If the majority element exists, then it's unique. 
    Sort A using, say, Mergesort (running in time O(nlogn)).
    Now, if it exists, all appearances of the majority element (call it x) form
    a continuous subsequence of A which has to pass through index n / 2.
    So, our only candidate for x is now A[n / 2], so we may simply return it.
    Overall this algorithms takes time O(nlogn), as required.


(b) First, consider the auxiliary function:
COUNT(A, i, j, val)
Input: An array with 2 indices i < j, a value val 
Output: The number of ks such that i <= k < j and A[k] = val
    
    cnt = 0
    for all k in [i, j)
        cnt += (A[k] == val)
    return cnt

Then, we solve the problem as follows:
MAJORITY(A, i, j)
Input:  An array with 2 indices i < j
Output: The majority element of A[i .. j), if it exists, or an
        arbitrary value otherwise

    // Edge case, length 1 array
    if i + 1 == j
        return A[i]

    p = (i + j) / 2
    
    m1 = MAJORITY(A, i, p + 1)
    m2 = MAJORITY(A, p, j)

    // The majority element of A[i .. j) is m1 or m2, whichever
    // appears more often in A[i .. j)
    cnt_m1 = COUNT(A, i, j, m1)
    cnt_m2 = COUNT(A, i, j, m2)
    if cnt_m1 > cnt_m2
        return m1
    else
        return m2

Correctness:
Assuming inductively that MAJORITY works for smaller arrays, we need to show
that it also works for A[i .. j). We have been given the hint that, in our
situation, if A[i .. j) does have a majority element, then it has to be either
of m1 or m2. Knowing this, together with the fact that the majority element has
to be the most frequent element, returning m1 or m2, whichever is more frequent
is surely the right choice.

Complexity O(nlogn) by the master theorem: T(n) = 2 * T(n / 2) + O(n).


(c) (i) Denote by x all appearances of the majority element in A and by y all
        other elements of A (e.g. after this A look like this: x x y x x y).
        
        Consider the "disbalance" of A - the difference between the number of
        x's and the number of y's in A. (i.e. D = cnt_x - cnt_y > 0)
        
        Imagine that instead of building B straight away, we instead
        successively change A so that at the end it resembles B (i.e. erase
        elements from A every time we consider a new pair of elements).

        Consider processing all x y and y x pairs before the x x and y y pairs:
            - For every x y / y x pair we will remove both the x and the y, so
              D will stay constant.

        Now, D > 0 and all we're left with are x x and y y pairs.
        What will happen is that we'll remove exactly one member of each pair,
        practically performing cnt_x /= 2, cnt_y /= 2, implying D /= 2.
        It was assumed that D was initially positive and it was shown that
        it ought to still be positive at the end, implying that x will still
        be a majority element after all changes have taken place (since D > 0
        is the defining property of a majority element).
        
   (ii) Assume for a contradiction that majority element of A[0 .. n)
        (call it m) is neither y nor the majority element of A[0 .. (n - 1))
        (call it x).
        Since m is a majority element, cnt_m > n / 2.
        Since m != x and m != y, we have that cnt_m <= (n - 1) / 2, a
        contradiction with the fact that cnt_m > n / 2.
        
  (iii) MAJORITY(A, n)
        Input: An array A of length n
        Output: The majority element of A, if it exists, or
                otherwise any element of A
        
            // Edge case, length 1 array
            if n == 1
                return A[0]
    
            // Even length array
            if n % 2 == 0
                B = [] // Create a length 0 array B
                m = 0  // B's length, initially 0
                for i in [0, n) by 2
                    if A[i] == A[i + 1]
                        B[m ++] = A[i]
                return MAJORITY(B, m)
            else
                // Similar to (b) find the two candidates and test for which
                // one of them is more frequent in A[0, n)
                x = MAJORITY(A, n - 1)
                y = A[n - 1]
    
                cnt_x = COUNT(A, 0, n, x)
                cnt_y = COUNT(A, 0, n, y)
    
                if cnt_x > cnt_y
                    return x
                else
                    return y
    
        As a summary, I've simply implemented a recursive function following
        the two reduction rules we've seen in (i) and (ii), the even rule
        always at least halving n, while the odd rule always decreases n by 1.

   (iv) By looking closely at the values of n produced by the successive calls
        of MAJORITY we realize that every two calls we are guaranteed to have
        an even n, meaning that we are guaranteed to halve n every two calls.
        (strong result: the total number of calls is equal to ~log_2(n) + the
        total number of set bits when writing n in base 2).
        
        Call a "phase" of the algorithm the parts of its execution between two
        successive halvings of n.
 
        With these in mind, we can write T(n) = T(n / 2) + O(n), because it
        takes at most 2 calls to reach a new phase and linear time overall
        between any 2 successive phases of the algorithm.
        Via the Master Theorem, the algorithm runs in time O(n).


................................................................................
-- Sheet 3 --


Question 5

(a) P0 = In
    P1 = E + P0 (assuming no self loops)


(b) Define boolean addition       a + b = a OR  b, where a, b in {false, true}.
    Define boolean multiplication a * b = a AND b, where a, b in {false, true}.
    Define boolean matrix multiplication in the usual manner using the
    definitions for addition and multiplication provided above.

    The, P(k + 1) = Pk * P1 is the required relation.


(c) To compute the Reflexive Transitive Closure of a graph with adjacency matrix
    E, simply compute P0, P1, P2, ... P(n - 1). P(n - 1) is the adjacency matrix
    of the Reflexive Transitive Closure of the graph).
   
    
(d) Now, improving the algorithm to stop after meeting the first m such that
Pm = P(m - 1) and setting P(n - 1) = Pm (here m will be ~the length of the
longest simple path in the Transitive Closure), the algorithm runs in time
O(n^3 * m), dominated by the m matrix multiplications, a tight bound.


(e) P(2k) = Pk^2
    Now the alternative algorithm is to compute P0, P1, P2, P4, P8, ...
    until P(2^(k - 1)) = P(2^k) (here 2^k is ~the first power of 2 larger than
    m).
    This algorithm has complexity O(n^3 * log(m)), which is tight and a factor
    of m / log(m) better than the previous solution.
    

................................................................................ 


Question 6

(a) Solving a problem by Dynamic programming is done by identifying a collection
of subprobems, tackling them in turn, smallest ones first, using the answer to
smaller problems to work out larger ones, until all of them are solved.
The principle of optimality has to be satisfied: The optimal solution to a
problem is a composite of optimal solutions to (some of) its subproblems.


(b) For a subset S of {0, ..., n - 1} let dp[S, j] = true / false
    depending on whether there is a hamiltonian path of the subgraph induced by
    S ending in j (j in S).

    dp[{i}, i] = true
    dp[S, i] = OR over k in S - {i} of dp[S - {i}, k]

    The dp values will be calculated in increasing order of cardinality of S,
    breaking ties arbitrarily.

    Time complexity O(2^n * n^2), as there are 2^n * n states, each of which
    is processed in linear time - O(n).

    Implementation-wise the S values will be stored as bitmasks (aka unsigned
    integers) and dp will be a 2^n by n array.


(c) A topological sort of a DAG is a total ordering of the vertices of the DAG
">" such that if (u, v) in E then u > v

The topological sort may not be unique, consider the graph:
V = {1, 2, 3}, E = {1 -> 2, 1 -> 3}.

This admits both 1 > 2 > 3 and 1 > 3 > 2 as topological sorts.


(d) "If" If a DAG G has a Hamiltonian Path then its topological sort is unique:
    Assume WLOG that the Hamiltonian Path is 1 -> 2 -> ... -> N. In the
    condition for an ordering to be a topological sort this directly translates
    to 1 > 2 > 3 > 4 ... > N, thus leaving 1 > 2 > ... > N as the only possible
    valid topological sort.

    "Only If" Proof by contrapositive:
    If a DAG doesn't have a Hamiltonian Path, then it must admit at least two
    different topological sorts.
    Assume WLOG that 1 > 2 > 3 > ... > N is a valid topological sort of G.
    Because G doesn't admit Hamiltonian Paths, it follows that there exists a
    v such that v -> v + 1 is not an edge in G.
    Then, 1 > 2 > 3 > ... > v - 1 > v + 1 > v > v + 2 > v + 3 > ... > N is
    another valid topological sort, thus completing the proof that the
    topological sort is not unique.
   
    
(e) Based on (d) the following very simple algorithm works:
Compute one topological sort of G, say WLOG 1 > 2 > 3 > ... > N and check in
linear time whether 1 -> 2, 2 -> 3, ..., N - 1 -> N are all edges in G,
if the answer is positive, then we've found a Hamiltonian Path and we're done,
otherwise, as a result of (d), no such Hamiltonian Path may exist, so we're
again done.


................................................................................


Question 7

(a) A[i] >= A[Parent(i)] for all 0 < i < N


(b) Define D-HEAPIFY as an analogous to the usual operation:

D-HEAPIFY(A, pos)    
    // Compute the child of pos with the smallest key associated to it
    smallest = (-INF, -1)
    for i in [0, d)
        kid = Child(pos, i)
        if kid != null
            smallest = min(smallest, (A[kid], kid))
    
    // If A[pos] is too large to stand here, swap pos with the child with
    // the smallest key and recursively call D-HEAPIFY
    (node_val, node) = smallest
    if A[pos] > node_val
        swap(A[pos], A[node])
        D-HEAPIFY(A, node)

EXTRACT-MIN(A)
    min = A[0]
    swap(A[0], A[A.heap-size - 1])
    A.heap-size = A.heap-size - 1
    HEAPIFY(A, 0)
    return min

The time complexity is O(d * log_d(n)) because the height of the heap is
O(log_d(n)), O(d) time is spent on each call to D-HEAPIFY on finding the
minimum child and there are as many calls to D-HEAPIFY as there are levels
in the heap.


(c)

DECREASE-KEY(A, i, key)
    A[i] = key
    while i > 0 and A[i] < A[Parent(i)]
        swap(A[i], A[Parent(i)])
        i = Parent(i)

Time complexity O(log_d(n)).

INSERT(A, key)
    A[A.heap-size] = INF
    A.heap-size = A.heap-size + 1
    DECREASE-KEY(A, A.heap-size - 1, key)

Time complexity O(log_d(n)).


(d) For a fixed graph G, weighting function w: E -> R>=0, and source vertex s,
we are interested in the shortest paths between s and all other nodes in the
graph ("Single Source Shortest Paths"), where a path is considered shorter
whenever the total cost of the weights of the edges that are part of it is
smaller. Formally:

    Input: An undirected / directed graph G with non-negative
           weights w: E -> R>=0
           A source vertex s
    Output: For every vertex v, d[v] and pi[v] the total weight of the shortest
            path from s to v and the parent in the shortest paths tree.


(e) - At the beginning of the algorithm, MAKE-QUEUE is called exactly once.
    If implemented in the usual manner (calling D-HEAPIFY in reverse order
    of indices) this runs in time O(d|V|).
    - |V| times EXTRACT-MIN will be called in order to discover a new shortest
    path, totalling for O(|V| * d * log_d(|V|)).
    - |E| times an edge may be updated, causing a DECREASE-KEY operation,
    totalling for O(|E| * log_d(n)).
    - Overall, the whole algorithm takes O(d|V|log_d(|V|) + |E|log_d(|V|)) =
    O((d|V| + |E|)log_d(|V|))
      

................................................................................


Question 8

(a) A greedy algorithm works by always choosing as the next step the move that
offers the greatest immediate benefit; it does not reconsider previous
decisions, whatever new situation may arise.

Kruskal's algorithm is a greedy algorithm because, following the above recipe
it always selects the edge of minimum weight that can be added into the
current forest without closing a cycle. Once an edge has been added to the
forest it is never ever removed again.


(b) Consider n = 4 and:

i  si  ei
1   1  10
2   3   8
3   9  20
4  18  25

The maximal possible number of overlappings is 2 and it can be achieved at
t = 9. This way, we've tackled persons 1 and 3. Now persons 2 and 4 are still
left and their availability ranges are disjoint, so 2 more meetings will be
necessary.

In actuality, just 2 meetings overall are needed, at, say, t = 8 and t = 20.


(c) First, t = 8 will be picked and persons 1 and 2 will have the message
delivered to them. Then t = 20 will be picked and persons 3 and 4 will have the
message delivered to them. Only 2 meetings have been used, which is the optimum
on this particular test.


(d) Proceed by strong induction on the number of persons. Say we know for
sure that our algorithm is optimal for all groups of < n persons, let's prove
that it's also optimal for n persons.

Let t1 < t2 < ... < tk be the solution found by Greedy Strategy 2.
Say u1 < u2 < ... < uj is an optimal solution. Consider 3 cases:

    1. u1 > t1  - This means that there is no meeting in which person 1 takes
                  part, as t1 = e[1] < u1, so it's impossible that the optimal
                  solution looks like this, so we don't need to consider this
                  case at all.
    2. u1 = t1  - In this case we know for sure that there is at least one
                  solution starting with one meeting at time u1 = t1, so it's
                  safe to just remove all people which took part into this
                  first meeting and inductively conclude that j = k, since we
                  know our greedy strategy to be optimal on all sets of persons
                  of cardinality less than n.
    3. u1 < t1  - By increasing u1's value to t1, the set of persons which take
                  part in the first meeting gets larger (i.e. more people are
                  added in, none get removed), since t1 has been chosen as the
                  smalest e value. Therefore, this case actually also falls back
                  into case 2.
                 
(e) Sort the persons (s[i], e[i]) in increasing order of e[i], breaking ties
arbitrarily. Then, do a linear scan over the persons in this sorted order,
creating a new meeting at t = e[k], whenever k is the first person met who
hasn't heard the message yet.
In pseudocode, assuming the sorting has already taken place:

    ts = []     // All meeting times
    ts_size = 0 // The number of meetings

    last_meeting = -INF // The time of the last meeting which took place
    for i in [1, N]
        if s[i] > last_meeting
            ts[ts_size] = e[i]
            ts_size = ts_size + 1
            last_meeting = e[i]

The total time complexity is O(nlogn), dominated by sorting (for
which one has total freedom as to which O(nlogn) algorithm to use - e.g.
Mergesort).


................................................................................
-- Sheet 4 --


Question 5


(a) Consider the following binary search procedure:

SEARCH(A, l, r, v)
Input: An array A with 2 indices l < r and a value v.
Output: True / False depending on whether there exists l <= k < r s.t. A[k] = v  
    
    m = (l + r) / 2 // Compute the index of the middle element
    if v < A[m]     // If the searched value is smaller than the middle element
        return SEARCH(A, l, m, v) // Then, search in the left half of the array
    else if v == A[m] // Otherwise, if it perfectly matches
        return True   // Then, return True
    else              // Otherwise search in the right half of the array
        return SEARCH(A, m + 1, r, v)


The runtime is given by T(n) = 1 + T(n / 2), which is O(log(n)) by the
Master Theorem, but this is hiding away the constant factor, so one may directly
expand it out to get into T(n) = log2(n) + O(1).


(b) I will assume for simplicity that m > 0 (this can be decided at the
beginning by comparing m with 0 once), the other major case m < 0 being solvable
along the same reasoning lines with minor modifications.
Pick an integer constant base >= 2 and run the following 2 part algorithm:
    1. Find the smallest t such that base^t >= m:
        t = 0
        val = 1
        while val < m
            val = val * base
            t = t + 1

        This performs log_base(m) + O(1) ternary comparisons.

    2. Binary search for the actual value of m in the range (base^(t - 1),
       base^t]. This performs log_2(base^t - base^(t - 1)) + O(1) =
       log_2(base^(t - 1) * (base - 1)) + O(1) = (t - 1) *  log_2(base)
       + log_2(base - 1) + O(1)

    Overall, the algorithm requires log_base(m) + (t - 1) * log_2(base) +
    log_2(base - 1) + O(1) ternary comparisons. We know that t = log_base(m) +
    O(1), so the algorithm requires log_base(m) + (log_base(m) - 1 + O(1)) *
    log_2(base) + log_2(base - 1) + O(1) = log_base(m) + (log_base(m) + O(1)) *
    log_2(base) + log_2(base - 1) + O(1) = log_base(m) + log_base(m) *
    log_2(base) + log_2(base) * O(1) + log_2(base - 1) + O(1) =
    log_base(m) + log_base(m) * log_2(base) + O(1) =
    log_base(m) * (1 + log_2(base)) + O(1) =
    (log_2(m) / log_2(base)) * (1 + log_2(base)) + O(1) = 
    log_2(m) / log_2(base) + log_2(m) + O(1) =
    log_2(m) * (1 + log_base(2)) + O(1)
    
    Denote log_base(2) by eps (which can get arbitrarily small by letting base
    be arbitrarily large). We get:
    
    log_2(m) * (1 + eps) + O(1) ternary comparisons, as required. 


(c) Two pointers approach:

SOLVE(S, n, x)
Input: An ordered array of real numbers S, of length n.
       A real number x.
Output: True / False, whether or not there are 2 distinct
        elements a, b in S s.t. a + b = x.

    j = n - 1
    for i in [0, n)
        // While the sum is too large, decrease j
        while j > i and S[i] + S[j] > x
            j = j - 1
        // If we've reached equal elements,
        // then there's no solution.
        if S[i] == S[j]
            return False
        if S[i] + S[j] == x
            return True
    return False

This runs in O(n) time, as the inner loop runs in amortized constant time, as
j can only decrease.


................................................................................


Question 6

(a) 0 -> 1 -> 2 -> 4 -> 9 -> 10
    0 -> 2 -> 4 -> 9 -> 10
   
    
(b) The minimal number of jumps is 4.
    The solution is not unique:

    0 -> 2 -> 4 -> 8 -> 10
    0 -> 2 -> 4 -> 9 -> 10


(c) Dynamic Programming is an algorithm design strategy for optimization
problems. Solving a problem by Dynamic programming is done by identifying a
collection of subprobems, tackling them in turn, smallest ones first,
using the answer to smaller problems to work out larger ones,
until all of them are solved. The principle of optimality: The optimal
solution to a problem is a composite of optimal solutions to (some of) its
subproblems.


(d) Consider dp[pos] = The minimal number of moves necessary to reach position
n - 1 starting at position pos.
This array is defined for all 0 <= pos < n.

Initialization: 
Surely, dp[n - 1] = 0, as the goal has already been reached.

Recurrence:

dp[pos] = 1 + min over pos < k <= min(pos + A[pos], n - 1) of dp[k]

Here we basically brute forced what the next move from pos is and picked the
move which took fewer overall steps to complete the sequence.

We'll compute dp in decreasing order of pos, the final answer is found in dp[0]:
    dp[n - 1] = 0
    for pos in [n - 2, 0]
        dp[pos] = INF
        for k in (pos, min(pos + A[pos], n - 1)]
            dp[pos] = min(dp[pos], 1 + dp[k])
    return dp[0]

Total time complexity is O(n^2), which becomes tight for an array s.t. A[i]
= INF, for all 0 <= i < n.

Note: The following greedy algorithm is linear:
    // For simplicity
    for i in [0, n)
        A[i] = min(A[i], n - 1)

    l = r = ans = 0
    while r < n - 1
        best = 0
        for i in [l, r]
            best = max(best, A[i])
        l = r + 1
        r = best
        ans = ans + 1
    return ans


(e) Consider maintaining an extra array father[pos] = the k which minimized
dp[pos]. Then, by starting with x = 0, repeatedly performing x = father[x],
until x = n - 1, and recording the x's which we pass through we can produce the
answer sequence:

    dp[n - 1] = 0
    for pos in [n - 2, 0]
        dp[pos] = INF
        for k in (pos, min(pos + A[pos], n - 1)]
            if 1 + dp[k] < dp[pos]
                dp[pos] = 1 + dp[k]
                father[pos] = k
    
    seq = []    // The answer sequence, initially empty
    seq_l = 0   // The answer sequence's length
    x = 0       // The position we're currently at
    while x != n - 1
        seq[seq_l] = x
        seq_l = seq_l + 1
        x = father[x]

    return dp[0]


................................................................................


Question 7

(a) A divide-and-conquer algorithm is one which is based on the divide-and
conquer design strategy. The divide-and-conquer design strategy solves a problem
by:

1. Breaking the problem into "subproblems" - smaller instances of the same
problem
2. Recursively solving those subproblems
[Base case: If the subproblems become small enough, just solve them by brute
force.]
3. Appropriately combining their answers.


(b) (i) Assuming that merging two arrays, one of size n and another of size m
takes time n + m, the time it takes to do all the merges is 2n + 3n + 4n + ...
+ kn = n * (2 + 3 + ... + k) = n * (k * (k + 1) / 2 - 1) = O(nk^2).


(ii) Consider a function MERGE(A, i, j) which produces the merged version
of arrays [i, j) by recursively computing MERGE(A, i, m) and MERGE(A, m, j)
first (here m = (i + j) / 2) and then merging the two resulting arrays using
the linear time merging algorithm. This follows the recurrence T(k) = nk +
2T(k / 2). Via a Recursion Tree argument, where every level runs in O(nk) time
complexity and there are O(log2(k)) levels, we can conclude that the whole
algorithm runs in time O(nklog2(k)), a factor of k / log2(k) better than the
previous algorithm.


(c)   First, consider the auxiliary function:
COUNT(A, i, j, val)
Input: An array with 2 indices i < j, a value val 
Output: The number of ks such that i <= k < j and A[k] = val
    
    cnt = 0
    for all k in [i, j)
        cnt += (A[k] == val)
    return cnt

Then, we solve the problem as follows:
MAJORITY(A, i, j)
Input:  An array with 2 indices i < j
Output: The majority element of A[i .. j), if it exists, or an
        arbitrary value otherwise

    // Edge case, length 1 array
    if i + 1 == j
        return A[i]

    p = (i + j) / 2
    
    m1 = MAJORITY(A, i, p + 1)
    m2 = MAJORITY(A, p, j)

    // The majority element of A[i .. j) is m1 or m2, whichever
    // appears more often in A[i .. j)
    cnt_m1 = COUNT(A, i, j, m1)
    cnt_m2 = COUNT(A, i, j, m2)
    if cnt_m1 > cnt_m2
        return m1
    else
        return m2

All that we've got left to do is check whether the answer returned by MAJORITY
is, indeed, the MAJORITY element. This can be done in linear time by calling
COUNT on the answer once, at the end.

Correctness:
Assuming inductively that MAJORITY works for smaller arrays, we need to show
that it also works for A[i .. j). The important property this algorithm is based
on is that, if A[i .. j) does have a majority element, then it has to be either
of m1 or m2. Knowing this, together with the fact that the majority element has
to be the most frequent element, returning m1 or m2, whichever is more frequent
is surely the right choice.

Complexity O(nlogn) based on having the same recurrence T(n) = 2T(n / 2) + O(n)
as Mergesort (which in turn is easily proven via a Recursion Tree argument).


................................................................................


Question 8

(a) A spanning tree (ST) of a connected graph G is a subgraph of G, T = (V, E')
such that E' in E and T is a tree.
By definition, all spanning trees of G are, nevertheless, trees on |V| vertices,
thus all having exactly |V| - 1 edges, as all trees on |V| vertices do.


(b) Starting from his hometown, a salesman will conduct a journey in which each
target city is visited exactly once before he returns home.
Given the pairwise distances between the cities, what is the best order in
which to visit them, so as to minimize overall distance travelled.

Solution: For a subset {0, j} in S in {0, ..., n - 1} define
dp[S, j] = the length of the shortest simple path starting at
0 and ending at j.

dp[{0}, 0] = 0
dp[S, j] = min{C[S - {j}, i] + dist(i, j) where i in S and i != j}

To find the answer just compute min{dp[{0, ..., n - 1}, i] + dist(i, 0), where
0 <= i < n}.

dp will be computed in increasing order of cardinality of S,
with all nonsensical states being initialized to INF.

Time complexity: there are at most 2^n * n states, each one of them requiring
O(n) time to compute, thus raising the total time complexity to O(2^n * n^2).  


(c) Solve by Dynamic Programming:

dp[k, S1, S2] = True / False whether or not it's possible to partition a1 ... ak
into 3 parts A, B, C, such that A's sum is S1 and B's sum is S2 (C's sum thus
being uniquely determined).

Initialization:
dp[0, 0, 0] = True
dp[0, _, _] = False

Recurrence: (For simplicity dp[_][<0][_] = dp[_][_][<0] = False)
dp[k][S1][S2] =    dp[k - 1][S1][S2] 
                or dp[k - 1][S1 - ak][S2]
                or dp[k - 1][S1][S2 - ak]


dp is computed in increasing order of k, breaking ties arbitrarily.

By letting S = Sum(ai) / 3 the answer to our problem will be dp[n][S / 3][S / 3]
(or False if S is not divisible by 3).
In practice, some reindexing will be necessary for the second and third
dimensions in order to accommodate negative sums (simply work with S1' = S1 + S,
S2' = S2 + S as values for the second and third dimensions of dp).

The total time complexity is proportional to the number of states of the
recurrence - O(n * S^2).

